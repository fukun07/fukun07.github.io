---
layout: post

title: 一个教训：GPU上使用T.nnet.softmax()的注意事项

date: 2015-10-23 13:21:21

categories: Theano
---

### 10月23日补充

闲来想起了这件诡异的事情，觉得theano开发组应该不至于放着这么大的问题不管，于是又重新分析了一遍。从数值计算的角度来看，softmax本身的导数不太可能引起溢出（正向计算时减去最大的输入）。溢出主要来源于cross entropy部分，按说单独使用softmax也不太会造成nan的情况。

于是我尝试着运行：

	theano-cache clear

将theano的缓存文件都清除，之前的代码竟然正常了！当然，在损失函数部分还得softmax跟categorical_crossentropy联用才行。而且要注意的一点是，softmax和categorical_crossentropy要么同时出现在scan内部，要么同时在外部，如果分隔开的话theano就优化不了了，会导致一些数值问题。

之前写的也依旧放着吧。当时刚调出来bug情绪比较激动，今日回头去看有些地方并不正确，仅供参考了。

### 之前的

最近的一个项目中，发现theano竟然出现了梯度计算错误。最开始表现的现象为：程序在gpu上会出现NaN，而cpu上则没有这个问题。我猜想是梯度下降的时候出了问题，一查果然是gpu上的梯度出现了NaN。

这是个特别头疼的事情，因为bug已经超越了语法错误，必须要跟theano稍微底层一些的东西做斗争了。偏偏模型又比较复杂，涉及到CNN和LSTM，可能出现bug的地方实在是太多了。幸运的是，经过两天的排查，最终将数值问题锁定在了T.nnet.softmax()上。

假设我们的网络中softmax输出后面紧跟着cross entropy作为loss。按照theano的一般逻辑，它会首先求取cross entropy的梯度，传播到softmax，然后再求取softmax的梯度，继续反向传播。但实际上，这种特殊情况下我们可以有更加简单的梯度形式：

$$
\begin{align}

p &= softmax(z)\\

loss &= crossEntropy(p, y)\\

\frac{d(loss)}{dz} &= p - y\\

\end{align}
$$

theano的开发组也注意到了这个问题，所以当T.nnet.softmax()后面跟着T.nnet.categorical_crossentropy时，它会使用一个统一的函数来一次性处理掉二者的梯度：T.nnet.CrossentropySoftmaxArgmax1HotwithBias()。这样就提高了计算效率。

在我的程序中，最后输出使用了T.nnet.softmax()+T.nnet.categorical_crossentropy()，这是没有问题的。但是，在靠近输入的地方，我还使用了一次T.nnet.softmax()，这个家伙并没有直接跟着一个loss。那么，怎么就导致了数值问题呢？

关键问题在于，theano的cpu和gpu是两套底层代码，开发者们默认T.nnet.softmax()后面一定会跟着某种loss吧，所以就没有给T.nnet.softmaxGrad()实现gpu代码了！取而代之的是给各种联合梯度函数比如T.nnet.CrossentropySoftmaxArgmax1HotwithBias()实现了gpu代码，这不是坑爹呢！谁告诉你们softmax只会用在最终输出了？

T.nnet.softmaxGrad()没有实现gpu代码的后果是什么？就是使用gpu时，正向计算有softmax，而反向计算时程序就假装这一层没有存在过，梯度直接往前传了。。。

所以同志们，总结了两点教训如下：

- 为了提高计算效率，在输出层一定要softmax和自带的cross entropy配合使用，不要自己去实现cross entropy。

- 为了保证不被坑爹，不要单独使用T.nnet.softmax()，请用T.exp(z) / T.exp(z).sum(axis=-1)，这是有gpu代码的。如果报错，考虑dimshuffle()移动下维度。