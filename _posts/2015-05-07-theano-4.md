---
layout: post

title: 简明Theano教程（4）：简单的LSTM例子

date: 2015-05-07 10:30:43

categories: Theano
---


## 基本公式
LSTM全称Long Short Term Memory，是RNN的一个变种，目前不管在学术圈还是工业界都很火热。相比RNN来说，它增加了很多额外的结构。如果不是特别熟悉LSTM模型，看到这些额外的gate会眼花缭乱，为了方便编程，特意在最开头将基本的数学公式摆上。

LSTM随时间传递的状态有两个，一个是memory cell的状态c，另一个是hidden layer的状态h。尽管h是由c计算得到，但是它们在参与下一时刻的计算时有所分工，所以它们通常被共同视为LSTM的隐状态。我们用下标来表示时间，LSTM在一个时间单位内的计算过程是：

$$
\begin{align}

i_t &= sigmoid(W_i[x_{t},h_{t-1}] + b_i)\\

f_t &= sigmoid(W_f[x_{t},h_{t-1}] + b_f)\\

o_t &= sigmoid(W_o[x_{t},h_{t-1}] + b_o)\\

\tilde{c}_t &= tanh(W_c[x_{t},h_{t-1}] + b_c)\\

c_t &= i_t.*\tilde{c}_t+f_t.*c_{t-1}\\

h_t &= o_t.*tanh(c_t)\\

\end{align}
$$

其中，\\(x_t\\)是当前时刻的输入数据，\\(h_{t-1}\\)是前一时刻的隐状态。\\(i_t\\)、\\(f_t\\)、\\(o_t\\)分别代表t时刻的input gate、forget gate和output gate。\\(\tilde{c}_t\\)是给记忆模块的输入。

有了数学公式，接下来我们要做的就是用theano将这个公式表达出来。在真正建立LSTM模型之前，我们要给它设置一个任务。通过这个任务来完成我们这一章的例子。


## 记住第一个

假设有一串0、1序列长度为100，LSTM依次读取100个字符之后得到了\\(h_{100}\\)，现在，我们给LSTM设置的难题是，根据最后一个h还能记得第一个输入\\(x_1\\)么？如果LSTM在经历了长度为100的序列之后还能准确判断出第1个字符，说明t=100时的梯度很好地传递到了t=1时刻。这个任务是一个经典的测试LSTM的例子。

根据任务描述，我们来生成模拟数据。仿照上一章Logistic Regression的做法，我们创建一个管理数据的类，给脚本文件命名为ToyDataLSTM.py，并将以下代码复制进去：

	__author__ = 'fukun'
    
    import numpy
    import theano
    
    
    class ToyDataLSTM:
        """
        generate sequences consisting of 0 and 1
        """
    
        def __init__(self, data_size=1000, length=100):
            self.data_size = data_size
            self.current_index = -1
    
            # generate the simulating data
            self.data = []
            for i in xrange(self.data_size):
                self.data.append(numpy.random.randint(2, size=(length, 1)).astype(theano.config.floatX))
            self.shuffle()
    
        def get_data(self):
            # if exceed the maximum data number, then clear and shuffle again
            self.current_index += 1
            if self.current_index >= self.data_size:
                self.current_index = 0
                self.shuffle()
            return self.data[self.current_index]
    
        # give a random order to data
        def shuffle(self):
            numpy.random.shuffle(self.data)
    
我们看到，这个数据管理类比上一章的要复杂。原因是为了每次给出一个序列，我们要对整个数据流做一些更加智能的管理了。添加了两个功能：

- 对数据做shuffle，打乱数据之间的排列顺序，这对于随机梯度方法非常重要。

- 内部有一个变量current_index记录现在用到第几个数据了，一旦这个序号变量超过了数据个数，我们就要对其归零，并对数据重新shuffle，让整个过程再来一遍。

对于外部来说，只管不停地调用get_data()就好了，不用操心数据是怎么管理的。


## 用Theano写LSTM模型

明确了任务，我们也为LSTM模型单独建立一个脚本文件，取名为LSTM_model.py，并将下面的代码复制进去。

    __author__ = 'fukun'
    
    import numpy
    import theano
    import theano.tensor as T
    
    
    class LSTM_model(object):
        """
        An inplement of basic LSTM
        """
        def __init__(self, nx=1, nh=100):
            # define the input
            x = T.matrix('x')
            self.inputs = [x]
    
            # the random initialization range
            low = -0.02
            high = 0.02
            # input gate
            self.Wi = theano.shared(numpy.random.uniform(low, high, (nx+nh, nh)).astype(theano.config.floatX))
            self.bi = theano.shared(numpy.random.uniform(low, high, nh).astype(theano.config.floatX))
            # input modulator
            self.Wc = theano.shared(numpy.random.uniform(low, high, (nx+nh, nh)).astype(theano.config.floatX))
            self.bc = theano.shared(numpy.random.uniform(low, high, nh).astype(theano.config.floatX))
            # forget gate
            self.Wf = theano.shared(numpy.random.uniform(low, high, (nx+nh, nh)).astype(theano.config.floatX))
            self.bf = theano.shared(1.0 + numpy.random.uniform(low, high, nh).astype(theano.config.floatX))
            # output gate
            self.Wo = theano.shared(numpy.random.uniform(low, high, (nx+nh, nh)).astype(theano.config.floatX))
            self.bo = theano.shared(numpy.random.uniform(low, high, nh).astype(theano.config.floatX))
            # the last digit output
            self.Wy = theano.shared(numpy.random.uniform(low, high, (nh, nx)).astype(theano.config.floatX))
            self.by = theano.shared(numpy.random.uniform(low, high, nx).astype(theano.config.floatX))
    
            # initialization of c and h
            c0 = theano.shared(numpy.zeros(nh).astype(theano.config.floatX))
            h0 = theano.shared(numpy.zeros(nh).astype(theano.config.floatX))
    
            # add all above into params
            self.params = [self.Wi, self.bi,
                           self.Wc, self.bc,
                           self.Wf, self.bf,
                           self.Wo, self.bo,
                           self.Wy, self.by]
    
            # go through the input sequence
            ([self.c, self.h], updates) = theano.scan(fn=self.compute_one_step, sequences=[x], outputs_info=[c0, h0])
    
    		# define the costs
            t = x[0, 0]
            self.y = T.nnet.sigmoid(T.dot(self.h[-1, :], self.Wy) + self.by)
            loss = - T.mean(t*T.log(self.y + 1e-14) + (1-t)*T.log((1-self.y) + 1e-14))
            acc = T.mean(T.eq(T.round(self.y), t))
            self.costs = [loss, acc]
    
        def compute_one_step(self, x_t, c_tm1, h_tm1):
            x_and_h = T.concatenate([x_t, h_tm1], axis=0)
            i_t = T.nnet.sigmoid(T.dot(x_and_h, self.Wi) + self.bi)
            c_tilde = T.tanh(T.dot(x_and_h, self.Wc) + self.bc)
            f_t = T.nnet.sigmoid(T.dot(x_and_h, self.Wf) + self.bf)
            o_t = T.nnet.sigmoid(T.dot(x_and_h, self.Wo) + self.bo)
            c_t = i_t * c_tilde + f_t * c_tm1
            h_t = o_t * T.tanh(c_t)
            return c_t, h_t

整个脚本用来定义了一个类LSTM_model，该类有一个初始化函数，接受两个参数，分别是输入维度nx和c、h的维度nh。我们来看看初始化函数中做了些什么：

- 定义了输入x，它是一个矩阵，第0维代表时间，第1维代表特征。注意，因为之后用到的theano.scan()默认第0维代表时间，所以不管数据还是参数，都要遵守这个规范，将第0维作为时间维。如果每个时刻的输入是一个矩阵呢？那么我们就将x定义为T.tensor3()，其中第0维是时间，第1维和第2维构成了征程的矩阵输入。其它情况依次类推。

- 然后是一串长长的参数定义，别看密密麻麻一大块，其实结构很简单，就是不停地定义了各种权重矩阵W和偏置b，变量名的后一个字母表明了该参数的用途。i用于input gate，c用于调制输入，f用于forget gate，o用于output gate，y用于最后预测。在初始化forget gate的偏置时，我们加了个1.0，这样就能让forget gate在最初的时候都是接近1，处于完全打开状态，有助于梯度的顺畅流通。这个trick在LSTM最开始的文章就有提到，不过最近被人忘了，又被google提起来。

- 定义好c0和h0，这是c、h的初始状态。这里我们直接用全0向量作为初值。

- 把所有参数加到self.params里面，没什么特别要注意的。

- 接着，就是theano写时序模型最核心的操作theano.scan()了。它用函数compute_one_step()扫描了一遍序列x。而compute_one_step()里面定义的就是LSTM在一个时间单元内做的计算，就是本章第一节对应的那些数学公式。扫描完之后，我们得到两个序列self.c和self.h，其第0维是时间维。也就是说他们包含了全部时间对应的c和h。

- 最后来定义目标函数。前面说了任务是根据最后的信息来推断第一个字符，因此我们用最后的隐状态\\(h_{-1}\\)来计算，目标是x的第一个字符。用于优化的目标函数是cross-entropy，而另外一个监控的指标是预测准确率acc。

反过头来看，整个LSTM关于时序的计算就一句话：theano.scan()，其它部分都在定义参数和单步的计算过程。


## SGD优化

因为数据推送方式有了一点变化，不再是上一章的batch模式，所以我们将sgd_optimizer.py修改如下：

	__author__ = 'fukun'
    
    import numpy
    import theano
    import theano.tensor as T
    
    
    def sgd_optimizer(model, train_data, valid_data, lr=0.001, max_epoch=100):
        """
        Stochastic gradient descent
        """
        # compute gradient of loss, which is default to the 0th element in model.costs
        grad = T.grad(model.costs[0], model.params)
    
        # the update rules of SGD
        updates = [(p, p - lr * g) for p, g in zip(model.params, grad)]
    
        # training function
        train_func = theano.function(inputs=model.inputs, outputs=model.costs, updates=updates)
    
        # validation function
        valid_func = theano.function(inputs=model.inputs, outputs=model.costs)
    
        # training
        for epoch in xrange(max_epoch):
            train_costs = [[], []]
            for _ in xrange(train_data.data_size):
                loss, acc = train_func(train_data.get_data())
                train_costs[0].append(loss)
                train_costs[1].append(acc)
            valid_costs = [[], []]
            for _ in xrange(valid_data.data_size):
                loss, acc = valid_func(valid_data.get_data())
                valid_costs[0].append(loss)
                valid_costs[1].append(acc)
            print 'epoch=[%d]  training loss=%.4f, acc=%.2f%% || validation loss=%.4f, acc=%.2f%%' % \
                  (epoch, numpy.mean(train_costs[0]), 100*numpy.mean(train_costs[1]),
                   numpy.mean(valid_costs[0]), 100*numpy.mean(valid_costs[1]))
    
        return model
   
实际迭代求解的那一块代码做了修改，只是将监控过程调整了一下，具体见代码。


## 主函数

我们创建脚本LSTM_main.py，并复制如下代码：

	__author__ = 'fukun'
    
    from ToyDataLSTM import ToyDataLSTM
    from LSTM_model import LSTM_model
    from sgd_optimizer import sgd_optimizer
    
    # data dimension
    nx = 1
    nh = 20
    
    # generate the toy data
    print '... Generating toy data'
    train_data = ToyDataLSTM(10000, length=10)
    valid_data = ToyDataLSTM(500, length=10)
    
    # Logistic Regression model
    print '... Building model'
    model = LSTM_model(nx=nx, nh=nh)
    
    # optimize the model
    print '... Optimizing with SGD'
    model = sgd_optimizer(model, train_data, valid_data, lr=0.02, max_epoch=10000)

为了节省运行时间，我们把隐节点设置为20，而0、1序列长度设为10。如果想测试100的长度，记得把nh和数据个数都改大，不然不足以训练出一个记住100时间跨度的LSTM。


## 示例输出

经过4个epoch正确率即到100%。因为是SGD每次处理一个样本，所以参数更新了4*10000次（在主函数中我们生成的序列个数是10000）。

	... Generating toy data
	... Building model
	... Optimizing with SGD
	/Library/Python/2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility from scan_perform.scan_perform import *
	epoch=[0]  training loss=0.6945, acc=49.71% || validation loss=0.6941, acc=47.60%
	epoch=[1]  training loss=0.6943, acc=50.56% || validation loss=0.7026, acc=47.60%
	epoch=[2]  training loss=0.6942, acc=49.97% || validation loss=0.7064, acc=47.60%
	epoch=[3]  training loss=0.6926, acc=51.65% || validation loss=0.6692, acc=60.20%
	epoch=[4]  training loss=0.0802, acc=96.54% || validation loss=0.0010, acc=100.00%
	epoch=[5]  training loss=0.0006, acc=100.00% || validation loss=0.0004, acc=100.00%
	epoch=[6]  training loss=0.0003, acc=100.00% || validation loss=0.0003, acc=100.00%
	epoch=[7]  training loss=0.0002, acc=100.00% || validation loss=0.0002, acc=100.00%
	epoch=[8]  training loss=0.0002, acc=100.00% || validation loss=0.0002, acc=100.00%
	epoch=[9]  training loss=0.0001, acc=100.00% || validation loss=0.0001, acc=100.00%
	epoch=[10]  training loss=0.0001, acc=100.00% || validation loss=0.0001, acc=100.00%


## theano.scan()详解

类似于LSTM这样的序列结构在编程时会经常遇到，它们的基本框架是：有一个函数，还有若干序列，我们需要把这个函数反复用到这些序列上（按顺序来）。具体一点，对于LSTM来说，它有三个序列：x、c和h。每一步的操作是固定的，就是说我们将函数fn反复用于这三个序列上：

$$
(c_{t}, h{t}) = fn(x_{t}, c_{t-1}, h_{t-1})
$$

这个f就是LSTM的一步计算函数。

我们可以用theano.scan()来很方便地定义上述过程。首先来看scan()这个函数里面最重要的四个参数：

- *fn*: 这里接收一个函数句柄，也就是我们要反复进行的那个操作。

- *sequences*: 需要反复使用fn的序列，是外部输入的。

- *outputs_info*: 看起来像是info之类的用于设定细节的参数，但其实不是，不要被名字欺骗了。。。它同样是fn要操作的序列。它与*sequences*的区别在于它是由fn每步计算得到的，不需要外部输入。

- *non_sequences*: 看名字容易理解，fn会利用到它，但不会按照时序来逐步推进，它在fn中相当于一个常数。

对于LSTM来说，x要放在*sequences*里面，因为x是外部输入的，从数据中来。c和h要放在*outputs_info*中，因为c和h并不是外部输入的，每一步中新的c和h都是fn自己计算出来的。那么，*outputs_info*中到底给了个啥？一般来说，它里面给的是初始值。比如：

$$
(c_{0}, h_{0}) = f(x_{0}, c_{-1}, h_{-1})
$$

\\(x_0\\)是外部输入，而c\_{-1}和h\_{-1}是不可计算的，需要制定一个初始值才可以将迭代继续下去。一旦迭代开始了：

$$
(c_{1}, h_{1}) = f(x_{1}, c_{-1}, h_{-1})
$$

只需要输入x就行，c和h都能自己计算得到。所以x是每步都需要给的，放在*sequences*里面。c和h是每步计算出来，只需要给一个初始值来启动，在*outputs_info*里面给初始值就可以了。

然后，就来到一个关键问题了，fn()按照什么样的顺序来接收输入并给出输出呢？在之前的LSTM例子中，输入输出顺序是这样的：

	[c_t, h_t] = compute_one_step(x_t, c_tm1, h_tm1)

最self是类函数自带的，忽略就好。在每步迭代里面，scan()给conpute_one_step()三个输入，并接收2个输出。输入的排列优先级是这样的：

1. *sequences*优先级最高，按照给参数时的顺序排列在最前面，LSTM中就是x。

2. *outputs_info*优先级第二高，按照给参数时的顺序接在*sequences*后面，LSTM中就是c和h（给scan参数的时候是c在前，那么compute_one_step中也是c在前）。

3. *non_sequences*优先级最低，依次排在最后。

这样我们就知道compute_one_step()里面的参数应该怎么跟scan()的参数对应起来了。这里要尤其小心，一旦搞错顺序会导致scan不按照我们设想的那样去计算。

输出部分就比较简单了，因为只有*outputs_info*才需要接收上一步的结果，我们按照c在前h在后的顺序给出去就行。

另外就是时间维的问题。在scan中，时间维永远是第0维。也就是说，假如x是一个2维矩阵，c_init和h_init是向量（c和h的初始值），我们会这么计算：


	[c0, h0] = compute_one_step(x[0,:], c_init, h_init)
	[c1, h1] = compute_one_step(x[1,:], c0, h0)
	[c2, h2] = compute_one_step(x[2,:], c1, h1)
	...

 
           

最后附上[官网的说明](http://deeplearning.net/software/theano/library/scan.html)。