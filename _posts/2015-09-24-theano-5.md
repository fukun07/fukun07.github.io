---
layout: post

title: 一个教训：GPU上使用T.nnet.softmax()的注意事项

date: 2015-05-07 10:30:43

categories: Theano
---


最近的一个项目中，发现theano竟然出现了梯度计算错误。最开始表现的现象为：程序在gpu上会出现NaN，而cpu上则没有这个问题。我猜想是梯度下降的时候出了问题，一查果然是gpu上的梯度出现了NaN。

这是个特别头疼的事情，因为bug已经超越了语法错误，必须要跟theano稍微底层一些的东西做斗争了。偏偏模型又比较复杂，涉及到CNN和LSTM，可能出现bug的地方实在是太多了。幸运的是，经过两天的排查，最终将数值问题锁定在了T.nnet.softmax()上。

假设我们的网络中softmax输出后面紧跟着cross entropy作为loss。按照theano的一般逻辑，它会首先求取cross entropy的梯度，传播到softmax，然后再求取softmax的梯度，继续反向传播。但实际上，这种特殊情况下我们可以有更加简单的梯度形式：
$$
\begin{align}

p &= softmax(z)\\

loss &= cross_entropy(p, y)

\frac{d(loss)}{dz} = p - y

\end{align}
$$

theano的开发组也注意到了这个问题，所以当T.nnet.softmax()后面跟着T.nnet.categorical_crossentropy时，它会使用一个统一的函数来一次性处理掉二者的梯度：T.nnet.CrossentropySoftmaxArgmax1HotwithBias()。这样就提高了计算效率。

在我的程序中，最后输出使用了T.nnet.softmax()+T.nnet.categorical_crossentropy()，这是没有问题的。但是，在靠近输入的地方，我还使用了一次T.nnet.softmax()，这个家伙并没有直接跟着一个loss。那么，怎么就导致了数值问题呢？

关键问题在于，theano的cpu和gpu是两套底层代码，开发者们默认T.nnet.softmax()后面一定会跟着某种loss吧，所以就没有给T.nnet.softmaxGrad()实现gpu代码了！取而代之的是给各种联合梯度函数比如T.nnet.CrossentropySoftmaxArgmax1HotwithBias()实现了gpu代码，这不是坑爹呢！谁告诉你们softmax只会用在最终输出了？

T.nnet.softmaxGrad()没有实现gpu代码的后果是什么？就是使用gpu时，正向计算有softmax，而反向计算时程序就假装这一层没有存在过，梯度直接往前传了。。。

所以同志们，总结了两点教训如下：

- 为了提高计算效率，在输出层一定要softmax和自带的cross entropy配合使用，不要自己去实现cross entropy。

- 为了保证不被坑爹，不要单独使用T.nnet.softmax()，请用T.exp(z) / T.exp(z).sum(axis=-1)，这是有gpu代码的。如果报错，考虑dimshuffle()移动下维度。