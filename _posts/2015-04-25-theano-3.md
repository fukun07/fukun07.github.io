---
layout: post

title: 简明Theano教程（3）：一个实际的Logistic Regression例子

date: 2015-04-25 18:20:17

categories: Theano
---

## 整体架构

对于一个机器学习任务来说，通常可以分为三个部分：

- 数据
- 模型
- 优化

为了使代码结构清晰，提高代码的复用性，我们的例子也按照这三方面进行划分。这样一来，主文件反而变得简单。我们创建一个.py文件，取名为LR_main.py，作为我们的主文件，并将下面代码复制进去：

	from ToyData import ToyData
    from LR_model import LR_model
    from sgd_optimizer import sgd_optimizer
    
    # data dimension
    nx = 100
    
    # generate the toy data
    print '... Generating toy data'
    train_data = ToyData((4000, nx))
    valid_data = ToyData((1000, nx))
    
    # Logistic Regression model
    print '... Building model'
    model = LR_model(nx=nx)
    
    # optimize the model
    print '... Optimizing with SGD'
    model = sgd_optimizer(model, train_data, valid_data, lr=0.0001, max_epoch=100)
    
对于数据、模型和优化这三个子模块，我们都用单独的文件来写。只要规定好彼此之间的接口，就能像拼积木一样在不同的数据、不同的模型、不同的优化方法间做自由组合。比如，对于优化来说，其实并不需要知道具体的模型是什么，只要能得到目标函数和梯度即可。

下面，我们分别介绍三个子模块怎么写。

## 数据管理

创建一个专门用于管理数据的类，其核心目标是把脏活留给自己，让后续模块省心。一般来说，这个类负责的工作有：

- 获取数据，可以读硬盘也可以自己生成
- 对不同源的数据进行合并管理，比如既有图像数据又有文本数据
- 对数据随机排序
- 提供 高度格式化的minibatch数据，使优化模块能够每调用一次获取一个minibatch

下面是本例中的数据管理类，叫做ToyData，由随机生成的两类正态分布数据构成。因为规模较小，所以没有对数据进行随机排序，也没有分minibatch。创建一个.py文件，取名为ToyData.py，将下面代码复制进去。

    import numpy
    import theano
    
    class ToyData:
        """
        There are two categories in toy data.
        The first category of data center on -1.0, for all the dimensions.
        The second category of data center on 1.0, for all the dimensions.
        The variances are both 1.0
    
        A method called get_data() can return the data the label
        """
    
        def __init__(self, data_size=(1000, 2)):
            x1 = numpy.random.normal(-1.0, 1.0, data_size).astype(theano.config.floatX)
            x2 = numpy.random.normal(1.0, 1.0, data_size).astype(theano.config.floatX)
            self.data = numpy.concatenate([x1, x2], axis=0)
            label = [0] * data_size[0]
            label.extend([1] * data_size[0])
            self.label = numpy.array(label).astype('int32')
    
        def get_data(self):
            return self.data, self.label

有了数据管理类，模型和优化模块的形式就能很好地固定下来，它们只管从这里拿到一个又一个minibatch，不需要操心数据怎么读取，怎么随机化，这些都是数据管理类内部的细节。

## 模型定义

在定义模型阶段，我们也不希望过多考虑具体的数据形式和优化方法，最好能够按照数学设计自然而然地将模型写出来。

创建一个.py文件并取名为LR_model.py，将下面代码复制进去：

	import numpy
    import theano
    import theano.tensor as T
    
    class LR_model:
        """
        The Logistic Regression model. Parameters in shared variables, others in symbolic variables
        Note that there is no actual data in this definition
        """
        def __init__(self, nx):
            """
            :param nx: the input dimension
            """
            # model parameters
            self.W = theano.shared(numpy.random.uniform(-0.02, 0.02, (nx, 1)).astype(theano.config.floatX))
            self.b = theano.shared(numpy.random.uniform(-0.02, 0.02, 1).astype(theano.config.floatX))
    
            # input and label
            x = T.matrix('x')
            y = T.ivector('y')
    
            # describe the computation
            self.predict = T.nnet.sigmoid(T.dot(x, self.W) + self.b).flatten()
            loss = - T.mean(y * T.log(self.predict) + (1-y) * T.log(1-self.predict))
            acc = T.mean(T.eq(y, T.round(self.predict)))
    
            # parameter, input and cost interfaces
            self.params = [self.W, self.b]
            self.inputs = [x, y]
            self.costs = [loss, acc]

我们来解读一下这段代码。它仅由一个类构成，这个类就是一个LR模型，其定义都放在了初始化函数中。

1. 首先定义的是LR的参数W和b，均用[-0.02, 0.02]的均匀分布来初始化。我们将数值转换为一个略微有点奇怪的类型：theano.config.floatX，这个类型默认为float64，但是在执行python文件时可以通过环境变量来更改。这就免去了我们选择float32还是float64的担忧，写代码阶段都一样的写着，具体运行的时候再根据实际来选择。
2. 然后定义了输入x是一个矩阵，而标签y是一个int32的向量。
3. 接着定义了LR相关的计算，得出两个cost，一个是用于训练的cross entropy，一个是用于检验性能的正确率。需要注意的是，对于符号变量不要采用NumPy本身的函数，而要使用theano.tensor里的同名函数。
4. 最后，我们将W和b封装到一个list里面，取名params。对于更复杂的模型，也应该将所有参数依次放到一个list里面，方便求导。我们把所有的输入都封装到一个list里面，取名inputs。这里的输入包括了特征、标签等。与数据管理类return的值顺序对应。我们还把所有的cost封装到一起，默认第一个是用于优化的目标函数，而后面跟的是想要查看的cost。


## 参数优化

依旧是创建一个.py文件，命名为sgd_optimizer.py。复制如下代码：

	import theano
    import theano.tensor as T
    
    def sgd_optimizer(model, train_data, valid_data, lr=0.001, max_epoch=100):
        """
        Stochastic gradient descent
        """
        # compute gradient of loss, which is default to the 0th element in model.costs
        grad = T.grad(model.costs[0], model.params)
    
        # the update rules of SGD
        updates = [(p, p - lr * g) for p, g in zip(model.params, grad)]
    
        # training function
        train_func = theano.function(inputs=model.inputs, outputs=model.costs, updates=updates)
    
        # validation function
        valid_func = theano.function(inputs=model.inputs, outputs=model.costs)
    
        # training
        for epoch in xrange(max_epoch):
            loss1, acc1 = train_func(*train_data.get_data())
            loss2, acc2 = valid_func(*valid_data.get_data())
            print 'epoch=[%d]  training loss=%.4f, acc=%.2f%% || validation loss=%.4f, acc=%.2f%%' % \
                  (epoch, loss1, 100*acc1, loss2, 100*acc2)
    
        return model
  
我们尽量将优化抽象成一个独立的过程，对于模型，我们不用管具体的形式，只要有params和costs就够了；对于数据，我们也不用管具体的形式，只要每次调用get_data()来获取一个minibatch就够了。

在上面的代码中，我们先将model.costs[0]对model.params求梯度（回顾一下，我们约定过了costs第一个默认为目标函数），然后根据求得的梯度生成了updates。

接着，我们编译了两个函数，train_func()不仅计算costs，还会对参数进行更新，而valid_func()只计算costs而不更改参数。

在训练阶段，我们不停地调用get_data()来获取minibatch，然后传递到train_func()中进行SGD参数优化。注意train_func(*train_data.get_data())中的那个\*号。train_func()应该有两个参数x和y，而get_data()返回的是x和y组成的list，\*的作用是将其展开成独立的x和y传递给train_func()（只在传递函数参数时这个语法有效）。

## 运行

在命令行里进入对应目录，然后输入：

	python LR_main.py

前40个epoch的输出（每次运行不一样）：

	... Generating toy data
	... Building model
	... Optimizing with SGD
	epoch=[0]  training loss=0.7542, acc=15.21% || validation loss=0.7520, acc=16.35%
	epoch=[1]  training loss=0.7514, acc=16.11% || validation loss=0.7492, acc=17.80%
	epoch=[2]  training loss=0.7486, acc=17.35% || validation loss=0.7464, acc=18.65%
	epoch=[3]  training loss=0.7458, acc=18.80% || validation loss=0.7437, acc=20.05%
	epoch=[4]  training loss=0.7431, acc=19.78% || validation loss=0.7409, acc=21.55%
	epoch=[5]  training loss=0.7403, acc=21.19% || validation loss=0.7382, acc=22.90%
	epoch=[6]  training loss=0.7376, acc=22.48% || validation loss=0.7355, acc=23.65%
	epoch=[7]  training loss=0.7349, acc=23.77% || validation loss=0.7328, acc=25.55%
	epoch=[8]  training loss=0.7322, acc=25.26% || validation loss=0.7301, acc=26.75%
	epoch=[9]  training loss=0.7295, acc=26.95% || validation loss=0.7275, acc=28.75%
	epoch=[10]  training loss=0.7268, acc=28.46% || validation loss=0.7248, acc=30.60%
	epoch=[11]  training loss=0.7242, acc=29.91% || validation loss=0.7222, acc=32.40%
	epoch=[12]  training loss=0.7215, acc=31.40% || validation loss=0.7195, acc=33.95%
	epoch=[13]  training loss=0.7189, acc=32.92% || validation loss=0.7169, acc=35.30%
	epoch=[14]  training loss=0.7163, acc=34.67% || validation loss=0.7143, acc=36.95%
	epoch=[15]  training loss=0.7137, acc=36.65% || validation loss=0.7117, acc=38.60%
	epoch=[16]  training loss=0.7111, acc=38.46% || validation loss=0.7092, acc=40.15%
	epoch=[17]  training loss=0.7085, acc=40.25% || validation loss=0.7066, acc=41.90%
	epoch=[18]  training loss=0.7059, acc=41.89% || validation loss=0.7041, acc=43.75%
	epoch=[19]  training loss=0.7034, acc=43.44% || validation loss=0.7015, acc=45.10%
	epoch=[20]  training loss=0.7008, acc=45.19% || validation loss=0.6990, acc=46.90%
	epoch=[21]  training loss=0.6983, acc=46.83% || validation loss=0.6965, acc=48.85%
	epoch=[22]  training loss=0.6958, acc=48.83% || validation loss=0.6940, acc=50.65%
	epoch=[23]  training loss=0.6933, acc=50.75% || validation loss=0.6915, acc=52.50%
	epoch=[24]  training loss=0.6908, acc=52.52% || validation loss=0.6890, acc=54.05%
	epoch=[25]  training loss=0.6883, acc=54.04% || validation loss=0.6866, acc=56.15%
	epoch=[26]  training loss=0.6858, acc=55.75% || validation loss=0.6841, acc=57.55%
	epoch=[27]  training loss=0.6834, acc=57.43% || validation loss=0.6817, acc=59.05%
	epoch=[28]  training loss=0.6810, acc=59.05% || validation loss=0.6793, acc=60.70%
	epoch=[29]  training loss=0.6785, acc=60.94% || validation loss=0.6768, acc=62.20%
	epoch=[30]  training loss=0.6761, acc=62.75% || validation loss=0.6744, acc=64.60%
	epoch=[31]  training loss=0.6737, acc=64.35% || validation loss=0.6720, acc=65.85%
	epoch=[32]  training loss=0.6713, acc=66.05% || validation loss=0.6697, acc=67.25%
	epoch=[33]  training loss=0.6689, acc=67.88% || validation loss=0.6673, acc=68.65%
	epoch=[34]  training loss=0.6666, acc=69.35% || validation loss=0.6650, acc=70.35%
	epoch=[35]  training loss=0.6642, acc=71.00% || validation loss=0.6626, acc=72.20%
	epoch=[36]  training loss=0.6619, acc=72.62% || validation loss=0.6603, acc=73.55%
	epoch=[37]  training loss=0.6595, acc=74.24% || validation loss=0.6580, acc=75.30%
	epoch=[38]  training loss=0.6572, acc=75.67% || validation loss=0.6557, acc=76.25%
	epoch=[39]  training loss=0.6549, acc=77.11% || validation loss=0.6534, acc=77.40%
	epoch=[40]  training loss=0.6526, acc=78.57% || validation loss=0.6511, acc=78.55%

